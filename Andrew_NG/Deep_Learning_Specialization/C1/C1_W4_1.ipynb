{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from testCases_v4a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-initialization\n",
    "### 3.1 - 2-layer neural network.\n",
    "**Exercise**: Create and initialize the parameters of the 2-layer neural network.\n",
    "\n",
    "**Solution**: I created a parameter initializer which by giving a list of numbers of units in each layer it randomly initializes the parameters W and constructs zeros of b parameters, by considering its size of output and input. which length of this list demonstrate the number of layers - 2 (minus input and outpus size)\n",
    "+ **N**: List of number of Units in each layer\n",
    "+ **len(N)**: Number of layers\n",
    "+ **parameters**: Dictionary of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(N:list):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    for i in range(1, len(N)):\n",
    "        parameters[f'W{i}'] = np.random.randn(N[i], N[i-1]) * 0.01\n",
    "        parameters[f'b{i}'] = np.zeros((N[i], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01624345, -0.00611756, -0.00528172],\n",
       "        [-0.01072969,  0.00865408, -0.02301539]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.01744812, -0.00761207]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = [3, 2, 1]\n",
    "initialize_params(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - L-layer Nueral Network\n",
    "**Exercise**: Implement initialization for an L-layer Neural Network. \n",
    "\n",
    "**Solution**: Using the previous implemented solution we can give any **value of L**, then randomly initialize its parameters, given the number of **Units** of each **Layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01624345, -0.00611756, -0.00528172, -0.01072969],\n",
       "        [ 0.00865408, -0.02301539,  0.01744812, -0.00761207],\n",
       "        [ 0.00319039, -0.0024937 ,  0.01462108, -0.02060141],\n",
       "        [-0.00322417, -0.00384054,  0.01133769, -0.01099891],\n",
       "        [-0.00172428, -0.00877858,  0.00042214,  0.00582815]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[-0.01100619,  0.01144724,  0.00901591,  0.00502494,  0.00900856],\n",
       "        [-0.00683728, -0.0012289 , -0.00935769, -0.00267888,  0.00530355]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.]])}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = [4, 5, 2]\n",
    "initialize_params(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Forward Propagation module\n",
    "### 4.1 - Linear Forward\n",
    "\n",
    "**Exercise**: Build the linear part of forward propagation.\n",
    "\n",
    "**Solution**: Since, $$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "+ Use the output of activated previous layer to **forward propagate** to next layer.\n",
    "+ Also cache the values of **A**, **W**, **b** of the current layer to use for **backward propagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    return W.dot(A) + b, (A, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z is: [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "# test the implementation\n",
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "print('Z is:', linear_forward(A, W, b)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Linear-Activation \n",
    "**Exercise**: Implement the forward propagation of the *LINEAR->ACTIVATION* layer. \n",
    "\n",
    "**Solution**: Give mathematical relation,\n",
    "$$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$$\n",
    "+ **activation g**: it can be range of functions:\n",
    "    - sigmoid function: (between -1, +1) $$\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$$\n",
    "    - tanh funtion: $$\\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$$\n",
    "    - softmax function: (between 0, +1) $$\\sigma(Z_j) = \\frac{e^{Z_j}}{ \\sum_{k=1}^{K}e^{-(Z_k)}}$$\n",
    "    - relu function: $$Relu(Z) = max(0, Z)$$\n",
    "    - leaky relu function $$LRelu(Z) = max(\\epsilon, Z)$$\n",
    "+ **linear_forward**: It is using linear activation function, so we call it **Z** and then feed it to activation function.\n",
    "+ **cache**: at last cache any current parameter to use it in **backward propagation** step.\n",
    "    - linear_cache: it is parameters **A, W, b** of current layer.\n",
    "    - activation_cache: it is output **Z** of current layer.\n",
    "+ **A_prev**: it is the **activated** output of previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "\n",
    "    if activation == 'sigmoid':\n",
    "        return 1/(1+np.exp(-Z)), (linear_cache, Z)\n",
    "    \n",
    "    elif activation == 'relu':\n",
    "        return np.maximum(0, Z), (linear_cache, Z)\n",
    "    \n",
    "    elif activation == 'lrelu':\n",
    "        return np.maximum(0.01, Z), (linear_cache, Z)\n",
    "    \n",
    "    elif activation == 'tanh':\n",
    "        return np.exp(Z) - np.exp(-Z) / np.exp(Z) + np.exp(-Z), (linear_cache, Z)\n",
    "    \n",
    "    else:  # if not specified it would be linear activation function\n",
    "        return Z, (linear_cache, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with sigmoid: A = [[0.96890023 0.11013289]]\n",
      "with relu: A = [[3.43896131 0.        ]]\n",
      "with lrelu: A = [[3.43896131 0.01      ]]\n",
      "with tanh: A = [[ 31.18564924 -57.08171647]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "activations = ['sigmoid', 'relu', 'lrelu', 'tanh']\n",
    "for activation in activations:\n",
    "    print(f'with {activation}: A =', linear_activation_forward(A_prev, W, b, activation)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - L-Layer Model\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **Figure 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>\n",
    "**Exercise**: Implement the forward propagation of the above model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    A = X\n",
    "    size = len(parameters)//2\n",
    "    caches = []\n",
    "\n",
    "    for layer in range(1, size):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[f'W{layer}'], parameters[f'b{layer}'], 'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    A_Last, cache = linear_activation_forward(A, parameters[f'W{size}'], parameters[f'b{size}'], 'sigmoid')\n",
    "    caches.append(cache)\n",
    "\n",
    "    return A_Last, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "A_Last, caches = L_model_forward(X, parameters)\n",
    "print(f\"AL = {A_Last}\")\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cost Function\n",
    "**Exercise**: Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A_Last, Y):\n",
    "    return (-1/Y.shape[1]) * (np.dot(Y, np.log(A_Last.T)) + np.dot((1 - Y), np.log(1-A_Last.T))).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is : 0.2797765635793422\n"
     ]
    }
   ],
   "source": [
    "Y, A_Last = compute_cost_test_case()\n",
    "\n",
    "print('cost is :', compute_cost(A_Last, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Backward Propagation module\n",
    "### 6.1 - Linear backward\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
    "**Exercise**: Use the 3 formulas above to implement linear_backward().(given dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
      " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
      " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
      " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
      " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
      "dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
      " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
      " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
      "db = [[-0.14713786]\n",
      " [-0.11313155]\n",
      " [-0.13209101]]\n"
     ]
    }
   ],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print(f\"dA_prev = {dA_prev}\")\n",
    "print(f\"dW = {dW}\")\n",
    "print(f\"db = {db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear activation backward\n",
    "If $g(.)$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$\n",
    "which is,$$dZ^{[l]} = W^{[l+1] T} dZ^{[l+1]} * g'(Z^{[l]}) \\tag{11}$$\n",
    "\n",
    "**Exercise**: Implement the backpropagation for the *LINEAR->ACTIVATION* layer. \n",
    "**Solution**: By calculating the derivative of each activation function we have,\n",
    "if $g(Z)=a$ then,\n",
    "+ **sigmoid**: $$g'(Z)=dZ=a(1-a)$$\n",
    "+ **tanh**: $$1-a^2$$\n",
    "+ **relu**:<br> $0$ if $z<0$, $1$ if $z>=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        dZ = dA * (1/(1+np.exp(-cache[1]))) * (1 - 1/(1+np.exp(-cache[1])))\n",
    "    \n",
    "    elif activation == 'relu':\n",
    "        dZ = np.where(cache[1] >= 0, dA, 0)\n",
    "    \n",
    "    elif activation == 'lrelu':\n",
    "        dZ = np.where(cache[1] >= 0, dA, 0.01)\n",
    "    \n",
    "    elif activation == 'tanh':\n",
    "        dZ = dA * 1 - np.power(((np.exp(cache[1]) - np.exp(-cache[1])) / (np.exp(cache[1]) + np.exp(-cache[1]))), 2)\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, cache[0])\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "-------------------\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n",
      "-------------------\n",
      "lrelu:\n",
      "dA_prev = [[ 0.44090989 -0.01057952]\n",
      " [ 0.37883606 -0.00909008]\n",
      " [-0.2298228   0.00551454]]\n",
      "dW = [[ 0.4533396   0.36950544 -0.11101633]]\n",
      "db = [[-0.20337892]]\n",
      "-------------------\n",
      "tanh:\n",
      "dA_prev = [[ 0.44273331  0.74825519]\n",
      " [ 0.38040277  0.64291152]\n",
      " [-0.23077325 -0.39002551]]\n",
      "dW = [[-0.13307594  0.67292997  0.33515262]]\n",
      "db = [[-0.56287443]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "activations = ['sigmoid', 'relu', 'lrelu', 'tanh']\n",
    "for activation in activations:\n",
    "    dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation)\n",
    "    print (f\"{activation}:\")\n",
    "    print (\"dA_prev = {}\".format(dA_prev))\n",
    "    print (\"dW = {}\".format(dW))\n",
    "    print (\"db = {}\\n-------------------\".format(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-Model Backward\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Figure 5** : Backward pass  </center></caption>\n",
    "\n",
    "**Exercise**: Implement backpropagation for the *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model.\n",
    "\n",
    "**Solution**: first calculate the dA and dZ of the **last layer** we get,\n",
    "$$dA = \\frac{A - Y}{A(1-A)}$$\n",
    "then using the estimated derivative of sigmoid activation function we have,\n",
    "$$dZ = \\frac{A - Y}{A(1-A)} * A(1-A) = A-Y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(A_Last, Y, caches):\n",
    "    grads = {}             # keep the gradients here\n",
    "    L = len(caches)        # number of layers\n",
    "    m = A_Last.shape[1]    # number of examples\n",
    "    Y = Y.reshape(A_Last.shape)  # Y has same shape as A_Last\n",
    "\n",
    "    grads[f'dA{L-1}'], grads[f'dW{L}'], grads[f'db{L}'] = linear_activation_backward(\n",
    "        -(np.divide(Y, A_Last) - np.divide(1 - Y, 1 - A_Last)),\n",
    "        caches[L-1], # last layer\n",
    "        'sigmoid'\n",
    "    )\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        grads[f'dA{l}'], grads[f'dW{l+1}'], grads[f'db{l+1}'] = linear_activation_backward(\n",
    "            grads[f'dA{l+1}'],\n",
    "            caches[l], # last layer\n",
    "            'relu'\n",
    "        )\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Update Parameters\n",
    "if $\\alpha$ is the learning rate then,\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "**Exercise**: Implement `update_parameters()` to update your parameters using gradient descent.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[f'W{l+1}'] = parameters[f'W{l+1}'] - learning_rate * grads[f'dW{l+1}']\n",
    "        parameters[f'b{l+1}'] = parameters[f'b{l+1}'] - learning_rate * grads[f'db{l+1}']\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "L = len(parameters) // 2\n",
    "for l in range(L):\n",
    "    print(f\"W{l+1} = {parameters[f'W{l+1}']}\")\n",
    "    print(f\"b{l+1} = {parameters[f'b{l+1}']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
